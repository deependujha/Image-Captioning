{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the dataset module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"../../datasets/results.csv\", delimiter=\"|\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.02, random_state=42)\n",
    "\n",
    "image_dir = \"../../datasets/flickr30k_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _my_train_and_test_dataloader(_datasets_dir, batch_size, testing_in_local=False):\n",
    "    \"\"\"\n",
    "    Function to get the train and test dataloaders\n",
    "    \"\"\"\n",
    "    caption_csv_file = f\"{_datasets_dir}/results.csv\"\n",
    "    image_dir = f\"{_datasets_dir}/flickr30k_images\"\n",
    "\n",
    "    df = pd.read_csv(caption_csv_file)\n",
    "    _train_df, _test_df = train_test_split(df, test_size=0.02, random_state=42)\n",
    "\n",
    "    train_dataset, test_dataset = get_train_test_dataset(_train_df, _test_df, image_dir)\n",
    "\n",
    "    if testing_in_local == False:\n",
    "        # train_sampler\n",
    "        # SageMaker data parallel: Set num_replicas and rank in DistributedSampler\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "            train_dataset, num_replicas=dist.get_world_size(), rank=dist.get_rank()\n",
    "        )\n",
    "    else:\n",
    "        train_sampler = None\n",
    "\n",
    "    train_loader, test_loader = get_train_test_dataloader(\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        train_sampler=train_sampler,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        _train_df,\n",
    "    )  # train_df is required to get the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_train_test_dataloader\n",
    "from training_script import _my_train_and_test_dataloader\n",
    "\n",
    "batch_size = 1\n",
    "data_dir = \"../../datasets/\"\n",
    "train_loader, test_loader = get_train_test_dataloader(train_df, test_df, batch_size=4)\n",
    "train_dataloader, test_dataloader, train_df = _my_train_and_test_dataloader(\n",
    "    data_dir, batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.batch_size, len(list(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_first_data = next(iter(train_loader))\n",
    "\n",
    "train_first_data[0].shape, train_first_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(train_first_data[0][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test the vision-transformer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vision_transformer_encoder import ViTEncoder\n",
    "\n",
    "# RANDOM_SEED = 42\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 20\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "PATCH_SIZE = 16\n",
    "IMG_SIZE = 224\n",
    "IN_CHANNELS = 3\n",
    "NUM_HEADS = 8\n",
    "DROPOUT = 0.001\n",
    "ADAM_WEIGHT_DECAY = 0\n",
    "ADAM_BETAS = (0.9, 0.999)\n",
    "ACTIVATION = \"gelu\"\n",
    "NUM_ENCODERS = 4\n",
    "EMBED_DIM = (PATCH_SIZE**2) * IN_CHANNELS  # 768\n",
    "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2  # 196\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTEncoder(\n",
    "    num_patches=NUM_PATCHES,\n",
    "    image_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_encoders=NUM_ENCODERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    in_channels=IN_CHANNELS,\n",
    "    activation=ACTIVATION,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image = torch.randn(BATCH_SIZE, 3, 224, 224).clip(0,1).to(device)\n",
    "print(f\"{BATCH_SIZE=}\")\n",
    "print(f\"{random_image.shape=}\")\n",
    "\n",
    "print(model(random_image).shape)  # BATCH_SIZE X (NUM_PATCHES+1) X EMBED_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(random_image[0].permute(1, 2, 0).cpu().detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Test the vocabulary module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from caption_vocab import MyVocab\n",
    "\n",
    "df = pd.read_csv(\"../../datasets/results.csv\", delimiter=\"|\")\n",
    "my_vocab = MyVocab(df=df, column_name=\" comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "token_arr = my_vocab.get_token_index_from_sentence(\n",
    "    \"Several men in hard hats are operating a giant pulley system .\",max_len=20\n",
    ")\n",
    "\n",
    "np.shape(token_arr), token_arr[0][10:]  # after 10th token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_arr = my_vocab.get_token_index_from_sentence(\n",
    "    [\"Several men in hard hats are operating a giant pulley system .\"], max_len=20\n",
    ")\n",
    "\n",
    "np.shape(token_arr), token_arr[0][10:]  # after 10th token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab.PAD_IDX, my_vocab.BOS_IDX, my_vocab.EOS_IDX, my_vocab.UNK_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(my_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab.get_sentence_from_indices(token_arr[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab.get_sentence_from_indices(token_arr.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocab\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.save(my_vocab, \"my_vocab.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasreen_vocab = torch.load(\"my_vocab.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasreen_vocab.get_sentence_from_indices(token_arr.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test the `Caption Generator Decoder` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dummy_encoder_output = torch.randn(4, 197, 256) # BATCH_SIZE X (NUM_PATCHES+1) X EMBED_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dummy_captions = [\n",
    "    \"Several men in hard hats are operating a giant pulley system .\",\n",
    "    \"operating a giant pulley system .\",\n",
    "    \"in hard hats are operating .\",\n",
    "    \"Several men in \",\n",
    "]\n",
    "meow_meow = my_vocab.get_token_index_from_sentence(\n",
    "        my_dummy_captions, max_len=20\n",
    "    )\n",
    "meow_meow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_padding_mask = my_vocab.create_padding_mask(meow_meow)\n",
    "my_subsequent_mask = my_vocab.create_square_subsequent_mask(20) # max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caption_generator_decoder import ImageCaptionDecoder\n",
    "\n",
    "TGT_VOCAB_SIZE = len(my_vocab)\n",
    "EMBED_DIM = 256\n",
    "NUM_HEADS = 8\n",
    "NUM_ENCODERS = 6\n",
    "DROPOUT = 0.1\n",
    "ACTIVATION = \"gelu\"\n",
    "TGT_MAX_LEN = 20\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_img_caption_decoder = ImageCaptionDecoder(\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    emb_size=EMBED_DIM,\n",
    "    nhead=NUM_HEADS,\n",
    "    num_decoder_layers=NUM_ENCODERS,\n",
    "    dropout=DROPOUT,\n",
    "    activation=ACTIVATION,\n",
    "    tgt_max_len=TGT_MAX_LEN,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = my_img_caption_decoder(\n",
    "    trg=meow_meow,\n",
    "    memory=dummy_encoder_output,\n",
    "    tgt_mask=my_subsequent_mask,\n",
    "    tgt_key_padding_mask=my_padding_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output.shape  # BATCH_SIZE X TGT_MAX_LEN X TGT_VOCAB_SIZE (4, 20, 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meow_meow.shape, dummy_encoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## inference_encoder_decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_script import inference_encoder_decoder_model\n",
    "\n",
    "my_inference_output = inference_encoder_decoder_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = torch.nn.NLLLoss(ignore_index=my_vocab.PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_real_output = torch.randint(0, TGT_VOCAB_SIZE, (4, 20)).to(device)\n",
    "dummy_real_output.shape, final_output.shape, dummy_real_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(final_output.permute(0, 2, 1), dummy_real_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_script import model_fn, predict_fn, output_fn\n",
    "\n",
    "MODEL_PATH = \"./model/\"\n",
    "\n",
    "my_trained_model = model_fn(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dummy_input = torch.rand(size=(3, 224, 224), dtype=torch.float32).clip(min=0, max=1)\n",
    "plt.imshow(dummy_input.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dummy_input = torch.rand(size=(1, 3, 224, 224), dtype=torch.float32).clip(min=0, max=1)\n",
    "plt.imshow(dummy_input[0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_output = enc_model(dummy_input)\n",
    "enc_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "\n",
    "image_path = \"./assets/1000268201.jpg\"\n",
    "my_alpha_img = read_image(image_path)\n",
    "my_alpha_img.shape, plt.imshow(my_alpha_img.permute(1,2,0)), type(my_alpha_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "test_transform = v2.Compose(\n",
    "    [\n",
    "        v2.Resize(size=(224, 224), antialias=True),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "new_girl_image = test_transform(my_alpha_img)\n",
    "\n",
    "# new_girl_image.shape, plt.imshow(new_girl_image.permute(1,2,0))\n",
    "plt.imshow(new_girl_image.permute(1, 2, 0)), new_girl_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nasreen_output = predict_fn(new_girl_image, model=my_trained_model, context=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nasreen_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
